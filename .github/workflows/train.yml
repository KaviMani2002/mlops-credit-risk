name: Train Multi Model with DVC

on:
  workflow_dispatch:
  push:
    paths:
      - 'src/**.py'
      - '.github/workflows/train.yml'

jobs:
  train-model:
    runs-on: self-hosted

    env:
      MLFLOW_TRACKING_URI: http://20.106.177.129:5000
      AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
      AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
      GITHUB_ACTIONS: true

    steps:
      - name: ğŸ“¥ Checkout repo
        uses: actions/checkout@v3

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: ğŸ“¦ Install dependencies safely
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip uninstall pyopenssl cryptography -y
          pip install "dvc[azure]==2.44.0"  # safe DVC version
          pip install azure-storage-blob

      - name: ğŸ” Verify DVC version
        run: dvc --version

      - name: âš™ï¸ Configure DVC Azure Remote
        run: |
          dvc remote add -d azureremote azure://dvcdata
          dvc remote modify azureremote connection_string "${{ secrets.AZURE_DVC_CONNECTION }}"

      - name: â¬‡ï¸ Pull data from Azure DVC remote
        run: dvc pull

      # ---------- DATA INGESTION ----------
      - name: ğŸ” Before Data Ingestion - Show working directory and files
        run: |
          echo "PWD before ingestion:"
          pwd
          echo "Files before ingestion:"
          ls -R

      - name: ğŸ“‚ Run data ingestion
        run: python src/data_ingestion.py

      - name: ğŸ” After Data Ingestion - Show working directory and files
        run: |
          echo "PWD after ingestion:"
          pwd
          echo "Files after ingestion:"
          ls -R

      # ---------- DATA CLEANING ----------
      - name: ğŸ” Before Data Cleaning - Show working directory and files
        run: |
          echo "PWD before cleaning:"
          pwd
          echo "Files before cleaning:"
          ls -R

      - name: ğŸ§¹ Run data cleaning
        run: python src/data_cleaning.py

      - name: ğŸ” After Data Cleaning - Show working directory and files
        run: |
          echo "PWD after cleaning:"
          pwd
          echo "Files after cleaning:"
          ls -R

      # ---------- FEATURE ENGINEERING ----------
      - name: ğŸ” Before Feature Engineering - Show working directory and files
        run: |
          echo "PWD before feature engineering:"
          pwd
          echo "Files before feature engineering:"
          ls -R

      - name: ğŸ—ï¸ Run feature engineering
        run: python src/feature_engineering.py

      - name: ğŸ” After Feature Engineering - Show working directory and files
        run: |
          echo "PWD after feature engineering:"
          pwd
          echo "Files after feature engineering:"
          ls -R

      # ---------- MODEL TRAINING ----------
      - name: ğŸš€ Train models and log to MLflow
        run: python src/train_multi_model.py

      - name: ğŸš€ Promote Models to Staging stage
        run: python src/promote_best_models.py
